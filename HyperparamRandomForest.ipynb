{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for optimizing different classifiers and ensembling them\n",
    "\n",
    "Here is how the algorithm works for training:\n",
    "- Transform the dataset by deleting and adding features (should be done by this point)\n",
    "- We create N (adjustable) sets of undersampled data (should be done by this point)\n",
    "- For each undersampled set, there will be a blend of classifiers:\n",
    "    - Tensorflow\n",
    "    - XGBoost\n",
    "    - RandomForest\n",
    "    - SVM\n",
    "    - ...\n",
    "- For each classifier for each undersampled set, we optimize the hyperparameters\n",
    "- After optimization, we do a small ensemble learning for each undersampled data\n",
    "- Every model is saved\n",
    "\n",
    "For testing:\n",
    "- Transform the dataset (should be done by this point)\n",
    "- Ensemble classification on the whole dataset (no undersampling)\n",
    "\n",
    "Generating and ensembling predictions\n",
    "- Each model generates the likelihood that the user will convert\n",
    "- Ensembling works by taking a weighted mean of all the votes, the weights being the accuracy of the model.\n",
    "\n",
    "\n",
    "## The dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xa but this version of numpy is 0x9",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xa but this version of numpy is 0x9"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype has the wrong size, try recompiling",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e561f13b42b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Gin/anaconda2/lib/python2.7/site-packages/sklearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__check_build\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0m__check_build\u001b[0m  \u001b[0;31m# avoid flakes unused variable error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Gin/anaconda2/lib/python2.7/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexternals\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Gin/anaconda2/lib/python2.7/site-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmurmurhash\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmurmurhash3_32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m from .validation import (as_float_array,\n\u001b[1;32m     12\u001b[0m                          \u001b[0massert_all_finite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mnumpy.pxd\u001b[0m in \u001b[0;36minit sklearn.utils.murmurhash (sklearn/utils/murmurhash.c:5029)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: numpy.dtype has the wrong size, try recompiling"
     ]
    }
   ],
   "source": [
    "# Imports and Global Vars => add any which are necessary\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sc\n",
    "import sklearn\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Preprocessing modules (Shouldn't be needed by now)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Gridsearching and Parameter Optimization\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# For efficient saving of models\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The data\n",
    "X_train_file = \"data/X_train.npz\"\n",
    "X_valid_file = \"data/X_valid.npy\"\n",
    "y_train_file = \"data/y_train.npz\"\n",
    "y_valid_file = \"data/y_valid.npy\"\n",
    "X_test_file = \"data/X_test.npy\"\n",
    "\n",
    "X_train_clusters = np.load(X_train_file)\n",
    "X_valid = np.load(X_valid_file)\n",
    "y_train_clusters = np.load(y_train_file)\n",
    "y_valid = np.load(y_valid_file)\n",
    "X_test = np.load(X_test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function for setting the parameters\n",
    "param_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "param_grid = {'clf__penalty': ['l1', 'l2'],\n",
    "              'clf__C': param_range,\n",
    "              \"clf__fit_intercept\": [True, False],\n",
    "              \"clf__kernel\": [\"rbf\", \"sigmoid\", \"poly\", \"linear\"],\n",
    "              \"clf__gamma\": param_range}\n",
    "\n",
    "def get_params(*args):\n",
    "    \"\"\"\n",
    "    Returns a list of a dictionary of parameter options\n",
    "    \n",
    "    Usage:\n",
    "        get_params('penalty', 'C', 'kernel')\n",
    "        \n",
    "    Returns:\n",
    "        [{\n",
    "            'clf__penalty': ...,\n",
    "            'clf__C': ...,\n",
    "            'clf__kernel': ...,\n",
    "        }]\n",
    "    \"\"\"\n",
    "    to_return = [{}]\n",
    "    for arg in args:\n",
    "        to_return[0][\"clf__\" + arg] = param_grid[\"clf__\" + arg]\n",
    "    \n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters for the gridsearch\n",
    "cv = 5 # Cross validation\n",
    "n_jobs = -1\n",
    "scoring = \"log_loss\"\n",
    "\n",
    "# Set up the pipeline of your classifier\n",
    "name = \"RandomForestClassifier\"\n",
    "pipe = Pipeline([(\"clf\", RandomForestClassifier()])\n",
    "\n",
    "# Get the params\n",
    "params = get_params(\"penalty\", \"C\")\n",
    "\n",
    "# The GridSearch\n",
    "gs_pipe = GridSearchCV(pipe, params, scoring=scoring, cv=cv, verbose=1, n_jobs=n_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_clusters.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# What we actually want to do is generate a gs_pipe for each undersampled set\n",
    "num_examples = len(X_train_clusters.keys())\n",
    "\n",
    "# Group the clusters\n",
    "X_trains = [X_train_clusters[\"arr_{0}\".format(i)] for i in range(num_examples)]\n",
    "y_trains = [y_train_clusters[\"arr_{0}\".format(i)] for i in range(num_examples)]\n",
    "\n",
    "# Create a gridsearch for each dataset\n",
    "gs_pipes = [gs_pipe for i in range(num_examples)]\n",
    "pipes = [pipe for i in range(num_examples)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now the fun part - hyperparameter search for each model for each undersample\n",
    "\n",
    "param_search = False\n",
    "\n",
    "X_y_pairs = zip(X_trains, y_trains)\n",
    "data_gs_pipe_pairs = zip(gs_pipes, X_y_pairs)\n",
    "data_pipe_pairs = zip(pipes, X_y_pairs)\n",
    "\n",
    "def fit_custom(pair):\n",
    "    gs_pipe, X_y_pair = pair\n",
    "    X_train, y_train = X_y_pair\n",
    "    gs_pipe.fit(X_train, y_train)\n",
    "    return gs_pipe.best_estimator_\n",
    "\n",
    "def fit_no_search(pair):\n",
    "    pipe, X_y_pair = pair\n",
    "    X_train, y_train = X_y_pair\n",
    "    pipe.fit(X_train, y_train)\n",
    "    return pipe\n",
    "\n",
    "# Let's do this in parallel\n",
    "from multiprocessing import Pool\n",
    "\n",
    "p = Pool(5)\n",
    "\n",
    "if param_search:\n",
    "    best_classifiers = p.map(fit_custom, data_gs_pipe_pairs)\n",
    "else:\n",
    "    best_classifiers = p.map(fit_no_search, data_pipe_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save each sklearn classifier in a folder called clfs\n",
    "filenames = [\"clfs/{name}_{num}.pkl\".format(name=name, num=i) for i in range(num_examples)]\n",
    "\n",
    "for fname, best_clf in zip(filenames, best_classifiers):\n",
    "    joblib.dump(best_clf, fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inference\n",
    "num_tests = X_test.shape[0]\n",
    "\n",
    "def vote():\n",
    "    predictions = np.zeros((num_tests,1))\n",
    "    for clf in best_classifiers:\n",
    "        probs = clf.predict_proba(X_test)\n",
    "        probs = np.reshape(probs, (num_tests, 1))\n",
    "        predictions = np.hstack(predictions, probs)\n",
    "    means = np.mean(predictions, axis=1)\n",
    "    return means\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = vote()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
