{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for optimizing different classifiers and ensembling them\n",
    "\n",
    "Here is how the algorithm works for training:\n",
    "- Transform the dataset by deleting and adding features (should be done by this point)\n",
    "- We create N (adjustable) sets of undersampled data (should be done by this point)\n",
    "- For each undersampled set, there will be a blend of classifiers:\n",
    "    - Tensorflow\n",
    "    - XGBoost\n",
    "    - RandomForest\n",
    "    - SVM\n",
    "    - ...\n",
    "- For each classifier for each undersampled set, we optimize the hyperparameters\n",
    "- After optimization, we do a small ensemble learning for each undersampled data\n",
    "- Every model is saved\n",
    "\n",
    "For testing:\n",
    "- Transform the dataset (should be done by this point)\n",
    "- Ensemble classification on the whole dataset (no undersampling)\n",
    "\n",
    "Generating and ensembling predictions\n",
    "- Each model generates the likelihood that the user will convert\n",
    "- Ensembling works by taking a weighted mean of all the votes, the weights being the accuracy of the model.\n",
    "\n",
    "\n",
    "## The dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Imports and Global Vars => add any which are necessary\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sc\n",
    "import sklearn\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Preprocessing modules (Shouldn't be needed by now)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Gridsearching and Parameter Optimization\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# For efficient saving of models\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The data\n",
    "X_train_file = \"X_train.npz\"\n",
    "X_valid_file = \"X_valid.npy\"\n",
    "y_train_file = \"y_train.npz\"\n",
    "y_valid_file = \"y_valid.npy\"\n",
    "X_test_file = \"X_test.npy\"\n",
    "\n",
    "X_train_clusters = np.load(X_train_file)\n",
    "X_valid = np.load(X_valid_file)\n",
    "y_train_clusters = np.load(y_train_file)\n",
    "y_valid = np.load(y_valid_file)\n",
    "X_test = np.load(X_test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function for setting the parameters\n",
    "param_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "param_grid = {'clf__penalty': ['l1', 'l2'],\n",
    "              'clf__C': param_range,\n",
    "              \"clf__fit_intercept\": [True, False],\n",
    "              \"clf__kernel\": [\"rbf\", \"sigmoid\", \"poly\", \"linear\"],\n",
    "              \"clf__gamma\": param_range}\n",
    "\n",
    "def get_params(*args):\n",
    "    \"\"\"\n",
    "    Returns a list of a dictionary of parameter options\n",
    "    \n",
    "    Usage:\n",
    "        get_params('penalty', 'C', 'kernel')\n",
    "        \n",
    "    Returns:\n",
    "        [{\n",
    "            'clf__penalty': ...,\n",
    "            'clf__C': ...,\n",
    "            'clf__kernel': ...,\n",
    "        }]\n",
    "    \"\"\"\n",
    "    to_return = [{}]\n",
    "    for arg in args:\n",
    "        to_return[0][\"clf__\" + arg] = param_grid[\"clf__\" + arg]\n",
    "    \n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters for the gridsearch\n",
    "cv = 5 # Cross validation\n",
    "n_jobs = -1\n",
    "scoring = \"log_loss\"\n",
    "\n",
    "# Set up the pipeline of your classifier\n",
    "name = \"RandomForest\"\n",
    "pipe = Pipeline([(\"clf\", RandomForestClassifier())])\n",
    "\n",
    "# Get the params\n",
    "params = get_params(\"penalty\", \"C\")\n",
    "\n",
    "# The GridSearch\n",
    "gs_pipe = GridSearchCV(pipe, params, scoring=scoring, cv=cv, verbose=1, n_jobs=n_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# What we actually want to do is generate a gs_pipe for each undersampled set\n",
    "num_examples = len(X_train_clusters.keys())\n",
    "\n",
    "# Group the clusters\n",
    "X_trains = [X_train_clusters[X_train_clusters[\"arr_{0}\".format(i)]] for i in range(num_examples)]\n",
    "y_trains = [y_train_clusters[y_train_clusters[\"arr_{0}\".format(i)]] for i in range(num_examples)]\n",
    "\n",
    "# Create a gridsearch for each dataset\n",
    "gs_pipes = [gs_pipe for i in range(num_examples)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now the fun part - hyperparameter search for each model for each undersample\n",
    "\n",
    "X_y_pairs = zip(X_trains, y_trains)\n",
    "data_gs_pipe_pairs = zip(gs_pipes, X_y_pairs)\n",
    "\n",
    "def fit_custom(gs_pipe, X_y_pair):\n",
    "    X_train, y_train = X_y_pair\n",
    "    gs_pipe.fit(X_train, y_train)\n",
    "    return gs_pipe.best_estimator_\n",
    "\n",
    "# Let's do this in parallel\n",
    "from multiprocessing import Pool\n",
    "\n",
    "p = Pool(5)\n",
    "best_classifiers = p.map(fit_custom, data_gs_pipe_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save each sklearn classifier\n",
    "filenames = [\"clfs/{name}_{num}.pkl\".format(name=name, num=i) for i in range(num_examples)]\n",
    "\n",
    "for fname, best_clf in zip(filenames, best_classifiers):\n",
    "    joblib.dump(best_clf, fname)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
